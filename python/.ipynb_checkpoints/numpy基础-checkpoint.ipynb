{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (1,3,6)\n",
    "a = [[2,3,4],[2,3,4],[2,3,4]]\n",
    "np.prod(a[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3 4\n",
      "2 3 4\n",
      "2 3 4\n"
     ]
    }
   ],
   "source": [
    "for x,y,z in a:\n",
    "    print(x,y,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot(a, b, out=None)\n",
      "\n",
      "Dot product of two arrays. Specifically,\n",
      "\n",
      "- If both `a` and `b` are 1-D arrays, it is inner product of vectors\n",
      "  (without complex conjugation).\n",
      "\n",
      "- If both `a` and `b` are 2-D arrays, it is matrix multiplication,\n",
      "  but using :func:`matmul` or ``a @ b`` is preferred.\n",
      "\n",
      "- If either `a` or `b` is 0-D (scalar), it is equivalent to :func:`multiply`\n",
      "  and using ``numpy.multiply(a, b)`` or ``a * b`` is preferred.\n",
      "\n",
      "- If `a` is an N-D array and `b` is a 1-D array, it is a sum product over\n",
      "  the last axis of `a` and `b`.\n",
      "\n",
      "- If `a` is an N-D array and `b` is an M-D array (where ``M>=2``), it is a\n",
      "  sum product over the last axis of `a` and the second-to-last axis of `b`::\n",
      "\n",
      "    dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "a : array_like\n",
      "    First argument.\n",
      "b : array_like\n",
      "    Second argument.\n",
      "out : ndarray, optional\n",
      "    Output argument. This must have the exact kind that would be returned\n",
      "    if it was not used. In particular, it must have the right type, must be\n",
      "    C-contiguous, and its dtype must be the dtype that would be returned\n",
      "    for `dot(a,b)`. This is a performance feature. Therefore, if these\n",
      "    conditions are not met, an exception is raised, instead of attempting\n",
      "    to be flexible.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "output : ndarray\n",
      "    Returns the dot product of `a` and `b`.  If `a` and `b` are both\n",
      "    scalars or both 1-D arrays then a scalar is returned; otherwise\n",
      "    an array is returned.\n",
      "    If `out` is given, then it is returned.\n",
      "\n",
      "Raises\n",
      "------\n",
      "ValueError\n",
      "    If the last dimension of `a` is not the same size as\n",
      "    the second-to-last dimension of `b`.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "vdot : Complex-conjugating dot product.\n",
      "tensordot : Sum products over arbitrary axes.\n",
      "einsum : Einstein summation convention.\n",
      "matmul : '@' operator as method with out parameter.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> np.dot(3, 4)\n",
      "12\n",
      "\n",
      "Neither argument is complex-conjugated:\n",
      "\n",
      ">>> np.dot([2j, 3j], [2j, 3j])\n",
      "(-13+0j)\n",
      "\n",
      "For 2-D arrays it is the matrix product:\n",
      "\n",
      ">>> a = [[1, 0], [0, 1]]\n",
      ">>> b = [[4, 1], [2, 2]]\n",
      ">>> np.dot(a, b)\n",
      "array([[4, 1],\n",
      "       [2, 2]])\n",
      "\n",
      ">>> a = np.arange(3*4*5*6).reshape((3,4,5,6))\n",
      ">>> b = np.arange(3*4*5*6)[::-1].reshape((5,4,6,3))\n",
      ">>> np.dot(a, b)[2,3,2,1,2,2]\n",
      "499128\n",
      ">>> sum(a[2,3,2,:] * b[1,2,:,2])\n",
      "499128\n",
      "matmul(a, b, out=None)\n",
      "\n",
      "Matrix product of two arrays.\n",
      "\n",
      "The behavior depends on the arguments in the following way.\n",
      "\n",
      "- If both arguments are 2-D they are multiplied like conventional\n",
      "  matrices.\n",
      "- If either argument is N-D, N > 2, it is treated as a stack of\n",
      "  matrices residing in the last two indexes and broadcast accordingly.\n",
      "- If the first argument is 1-D, it is promoted to a matrix by\n",
      "  prepending a 1 to its dimensions. After matrix multiplication\n",
      "  the prepended 1 is removed.\n",
      "- If the second argument is 1-D, it is promoted to a matrix by\n",
      "  appending a 1 to its dimensions. After matrix multiplication\n",
      "  the appended 1 is removed.\n",
      "\n",
      "Multiplication by a scalar is not allowed, use ``*`` instead. Note that\n",
      "multiplying a stack of matrices with a vector will result in a stack of\n",
      "vectors, but matmul will not recognize it as such.\n",
      "\n",
      "``matmul`` differs from ``dot`` in two important ways.\n",
      "\n",
      "- Multiplication by scalars is not allowed.\n",
      "- Stacks of matrices are broadcast together as if the matrices\n",
      "  were elements.\n",
      "\n",
      ".. warning::\n",
      "   This function is preliminary and included in NumPy 1.10.0 for testing\n",
      "   and documentation. Its semantics will not change, but the number and\n",
      "   order of the optional arguments will.\n",
      "\n",
      ".. versionadded:: 1.10.0\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "a : array_like\n",
      "    First argument.\n",
      "b : array_like\n",
      "    Second argument.\n",
      "out : ndarray, optional\n",
      "    Output argument. This must have the exact kind that would be returned\n",
      "    if it was not used. In particular, it must have the right type, must be\n",
      "    C-contiguous, and its dtype must be the dtype that would be returned\n",
      "    for `dot(a,b)`. This is a performance feature. Therefore, if these\n",
      "    conditions are not met, an exception is raised, instead of attempting\n",
      "    to be flexible.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "output : ndarray\n",
      "    Returns the dot product of `a` and `b`.  If `a` and `b` are both\n",
      "    1-D arrays then a scalar is returned; otherwise an array is\n",
      "    returned.  If `out` is given, then it is returned.\n",
      "\n",
      "Raises\n",
      "------\n",
      "ValueError\n",
      "    If the last dimension of `a` is not the same size as\n",
      "    the second-to-last dimension of `b`.\n",
      "\n",
      "    If scalar value is passed.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "vdot : Complex-conjugating dot product.\n",
      "tensordot : Sum products over arbitrary axes.\n",
      "einsum : Einstein summation convention.\n",
      "dot : alternative matrix product with different broadcasting rules.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The matmul function implements the semantics of the `@` operator introduced\n",
      "in Python 3.5 following PEP465.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "For 2-D arrays it is the matrix product:\n",
      "\n",
      ">>> a = [[1, 0], [0, 1]]\n",
      ">>> b = [[4, 1], [2, 2]]\n",
      ">>> np.matmul(a, b)\n",
      "array([[4, 1],\n",
      "       [2, 2]])\n",
      "\n",
      "For 2-D mixed with 1-D, the result is the usual.\n",
      "\n",
      ">>> a = [[1, 0], [0, 1]]\n",
      ">>> b = [1, 2]\n",
      ">>> np.matmul(a, b)\n",
      "array([1, 2])\n",
      ">>> np.matmul(b, a)\n",
      "array([1, 2])\n",
      "\n",
      "\n",
      "Broadcasting is conventional for stacks of arrays\n",
      "\n",
      ">>> a = np.arange(2*2*4).reshape((2,2,4))\n",
      ">>> b = np.arange(2*2*4).reshape((2,4,2))\n",
      ">>> np.matmul(a,b).shape\n",
      "(2, 2, 2)\n",
      ">>> np.matmul(a,b)[0,1,1]\n",
      "98\n",
      ">>> sum(a[0,1,:] * b[0,:,1])\n",
      "98\n",
      "\n",
      "Vector, vector returns the scalar inner product, but neither argument\n",
      "is complex-conjugated:\n",
      "\n",
      ">>> np.matmul([2j, 3j], [2j, 3j])\n",
      "(-13+0j)\n",
      "\n",
      "Scalar multiplication raises an error.\n",
      "\n",
      ">>> np.matmul([1,2], 3)\n",
      "Traceback (most recent call last):\n",
      "...\n",
      "ValueError: Scalar operands are not allowed, use '*' instead\n"
     ]
    }
   ],
   "source": [
    "x =[]\n",
    "x.append([1,2])\n",
    "x.append([1,3])\n",
    "x\n",
    "len(x)\n",
    "np.info(np.dot)\n",
    "np.info(np.matmul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4, 5, 5, 4, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with remapped shapes [original->remapped]: (3,4,5,6)->(3,4,5,newaxis,6) (5,4,6,3)->(5,4,newaxis,3,6) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-90abdacd58f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#[2,3,2,1,2,2]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with remapped shapes [original->remapped]: (3,4,5,6)->(3,4,5,newaxis,6) (5,4,6,3)->(5,4,newaxis,3,6) "
     ]
    }
   ],
   "source": [
    "a = np.arange(3*4*5*6).reshape((3,4,5,6))\n",
    "b = np.arange(3*4*5*6)[::-1].reshape((5,4,6,3))\n",
    "x=np.dot(a, b)#[2,3,2,1,2,2]\n",
    "print(x.shape)\n",
    "x=np.matmul(a,b)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_count: 0，loss:391591.4173101807, w:[5.392273  2.0922742]\n",
      "iter_count: 1，loss:1029.8389248505594, w:[5.604175  2.5221941]\n",
      "iter_count: 2，loss:5.198111942467187, w:[5.5645247 2.516246 ]\n",
      "iter_count: 3，loss:0.22802599568199952, w:[5.5383186 2.5329018]\n",
      "iter_count: 4，loss:0.3197404117319733, w:[5.511706  2.5482051]\n",
      "iter_count: 5，loss:0.3076517313460877, w:[5.485408 2.56341 ]\n",
      "iter_count: 6，loss:0.3015391705557937, w:[5.459383  2.5784523]\n",
      "iter_count: 7，loss:0.2952360110976792, w:[5.433631  2.5933375]\n",
      "iter_count: 8，loss:0.2891018912488653, w:[5.4081483 2.6080666]\n",
      "iter_count: 9，loss:0.2830648820914561, w:[5.3829327 2.6226416]\n",
      "iter_count: 10，loss:0.2771778439973073, w:[5.3579807 2.6370637]\n",
      "iter_count: 11，loss:0.2713849406998838, w:[5.3332906 2.6513352]\n",
      "iter_count: 12，loss:0.2657549909118156, w:[5.3088584 2.665457 ]\n",
      "iter_count: 13，loss:0.2602014091135061, w:[5.2846823 2.679431 ]\n",
      "iter_count: 14，loss:0.25477362535969, w:[5.2607594 2.6932588]\n",
      "iter_count: 15，loss:0.2494735440496355, w:[5.237087  2.7069416]\n",
      "iter_count: 16，loss:0.2442677833428665, w:[5.213662  2.7204814]\n",
      "iter_count: 17，loss:0.23918653180241234, w:[5.190483  2.7338793]\n",
      "iter_count: 18，loss:0.23421643148060536, w:[5.1675463 2.7471368]\n",
      "iter_count: 19，loss:0.22932474379523193, w:[5.14485   2.7602556]\n",
      "iter_count: 20，loss:0.2245448330533749, w:[5.122391 2.773237]\n",
      "iter_count: 21，loss:0.21987457382935682, w:[5.1001678 2.7860825]\n",
      "iter_count: 22，loss:0.21530291729188905, w:[5.078177  2.7987933]\n",
      "iter_count: 23，loss:0.21081298596645356, w:[5.0564165 2.811371 ]\n",
      "iter_count: 24，loss:0.20642107477784738, w:[5.0348835 2.8238173]\n",
      "iter_count: 25，loss:0.2021148378633894, w:[5.013576 2.836133]\n",
      "iter_count: 26，loss:0.19789386975320666, w:[4.9924917 2.84832  ]\n",
      "iter_count: 27，loss:0.19377446555739736, w:[4.971628  2.8603795]\n",
      "iter_count: 28，loss:0.18974421478110015, w:[4.950983  2.8723125]\n",
      "iter_count: 29，loss:0.18579051667535681, w:[4.9305544 2.8841207]\n",
      "iter_count: 30，loss:0.18193148454243785, w:[4.9103394 2.895805 ]\n",
      "iter_count: 31，loss:0.17812402837211266, w:[4.890336 2.907367]\n",
      "iter_count: 32，loss:0.17441338716253085, w:[4.8705425 2.918808 ]\n",
      "iter_count: 33，loss:0.17078855878377946, w:[4.850956 2.930129]\n",
      "iter_count: 34，loss:0.16722238488105504, w:[4.8315744 2.9413319]\n",
      "iter_count: 35，loss:0.16374372306108853, w:[4.812396  2.9524171]\n",
      "iter_count: 36，loss:0.16033092044178046, w:[4.7934184 2.9633865]\n",
      "iter_count: 37，loss:0.1569971635597307, w:[4.7746396 2.9742408]\n",
      "iter_count: 38，loss:0.15372236052064692, w:[4.7560573 2.9849815]\n",
      "iter_count: 39，loss:0.15051915898381266, w:[4.7376695 2.9956098]\n",
      "iter_count: 40，loss:0.14737853166185233, w:[4.7194743 3.0061269]\n",
      "iter_count: 41，loss:0.1443142842693851, w:[4.70147   3.0165339]\n",
      "iter_count: 42，loss:0.14131850657882752, w:[4.683654  3.0268316]\n",
      "iter_count: 43，loss:0.13837146140464032, w:[4.666024  3.0370216]\n",
      "iter_count: 44，loss:0.13548450529184777, w:[4.648579 3.047105]\n",
      "iter_count: 45，loss:0.13266318967158441, w:[4.6313167 3.057083 ]\n",
      "iter_count: 46，loss:0.12989649812331336, w:[4.614235  3.0669563]\n",
      "iter_count: 47，loss:0.12719038585736564, w:[4.5973325 3.0767262]\n",
      "iter_count: 48，loss:0.12454576685882786, w:[4.580607  3.0863938]\n",
      "iter_count: 49，loss:0.1219558934664834, w:[4.5640564 3.0959601]\n",
      "iter_count: 50，loss:0.11941258193075337, w:[4.547679  3.1054263]\n",
      "iter_count: 51，loss:0.11691902602207992, w:[4.531473  3.1147933]\n",
      "iter_count: 52，loss:0.1144779284035845, w:[4.515437  3.1240623]\n",
      "iter_count: 53，loss:0.1120931825749358, w:[4.499569  3.1332343]\n",
      "iter_count: 54，loss:0.10975780137164984, w:[4.483867  3.1423104]\n",
      "iter_count: 55，loss:0.10748529633111903, w:[4.4683294 3.1512911]\n",
      "iter_count: 56，loss:0.10523772707996104, w:[4.452955 3.160178]\n",
      "iter_count: 57，loss:0.10304968446337298, w:[4.437741  3.1689715]\n",
      "iter_count: 58，loss:0.1008936182416568, w:[4.422686  3.1776733]\n",
      "iter_count: 59，loss:0.09879324451554711, w:[4.407789 3.186284]\n",
      "iter_count: 60，loss:0.09674293961377262, w:[4.3930483 3.1948044]\n",
      "iter_count: 61，loss:0.09472594467364834, w:[4.378462  3.2032356]\n",
      "iter_count: 62，loss:0.0927550819450058, w:[4.364028  3.2115784]\n",
      "iter_count: 63，loss:0.09081634170264587, w:[4.3497453 3.2198339]\n",
      "iter_count: 64，loss:0.08892251015408548, w:[4.3356123 3.228003 ]\n",
      "iter_count: 65，loss:0.0870763678989897, w:[4.321627  3.2360866]\n",
      "iter_count: 66，loss:0.08526346978842193, w:[4.3077884 3.2440856]\n",
      "iter_count: 67，loss:0.08348648373039906, w:[4.2940946 3.2520006]\n",
      "iter_count: 68，loss:0.08174365330569126, w:[4.2805443 3.2598329]\n",
      "iter_count: 69，loss:0.08004303464275436, w:[4.2671356 3.2675831]\n",
      "iter_count: 70，loss:0.0783738974366861, w:[4.2538676 3.2752523]\n",
      "iter_count: 71，loss:0.07674705323159144, w:[4.2407384 3.282841 ]\n",
      "iter_count: 72，loss:0.07514139109426322, w:[4.2277465 3.2903504]\n",
      "iter_count: 73，loss:0.07357672805795445, w:[4.214891  3.2977812]\n",
      "iter_count: 74，loss:0.07204937323655612, w:[4.20217  3.305134]\n",
      "iter_count: 75，loss:0.07054632582218766, w:[4.189582  3.3124099]\n",
      "iter_count: 76，loss:0.06907087133007735, w:[4.177126  3.3196096]\n",
      "iter_count: 77，loss:0.06763543249820941, w:[4.1648   3.326734]\n",
      "iter_count: 78，loss:0.06622643394619226, w:[4.1526036 3.3337839]\n",
      "iter_count: 79，loss:0.06484959583394811, w:[4.140535  3.3407598]\n",
      "iter_count: 80，loss:0.06349971650289954, w:[4.1285925 3.3476624]\n",
      "iter_count: 81，loss:0.062172994313716486, w:[4.116775  3.3544931]\n",
      "iter_count: 82，loss:0.060880250051037, w:[4.105081 3.361252]\n",
      "iter_count: 83，loss:0.05960322917348239, w:[4.0935097 3.3679407]\n",
      "iter_count: 84，loss:0.05836764694552549, w:[4.0820594 3.374559 ]\n",
      "iter_count: 85，loss:0.05714859905132472, w:[4.0707293 3.381108 ]\n",
      "iter_count: 86，loss:0.05596345881236629, w:[4.0595174 3.3875885]\n",
      "iter_count: 87，loss:0.054794361636048415, w:[4.0484233 3.394001 ]\n",
      "iter_count: 88，loss:0.05365322612811724, w:[4.037445  3.4003465]\n",
      "iter_count: 89，loss:0.05253523713781178, w:[4.026582  3.4066255]\n",
      "iter_count: 90，loss:0.05143759250631011, w:[4.0158324 3.412839 ]\n",
      "iter_count: 91，loss:0.050371833531538505, w:[4.0051956 3.418987 ]\n",
      "iter_count: 92，loss:0.04932018800651568, w:[3.9946702 3.4250708]\n",
      "iter_count: 93，loss:0.04829023183265272, w:[3.984255  3.4310908]\n",
      "iter_count: 94，loss:0.04728543520105632, w:[3.973949 3.437048]\n",
      "iter_count: 95，loss:0.04630288235634216, w:[3.9637506 3.4429426]\n",
      "iter_count: 96，loss:0.04533605462050801, w:[3.9536593 3.4487755]\n",
      "iter_count: 97，loss:0.044392125700729965, w:[3.9436736 3.4545474]\n",
      "iter_count: 98，loss:0.04346874410450109, w:[3.9337924 3.4602587]\n",
      "iter_count: 99，loss:0.04255955275218585, w:[3.9240146 3.4659104]\n",
      "iter_count: 100，loss:0.04167433704968462, w:[3.9143393 3.471503 ]\n",
      "iter_count: 101，loss:0.04081088455245517, w:[3.9047651 3.4770367]\n",
      "iter_count: 102，loss:0.03995393935396024, w:[3.8952913 3.4825127]\n",
      "iter_count: 103，loss:0.03912283564670398, w:[3.8859167 3.4879315]\n",
      "iter_count: 104，loss:0.038311494570867216, w:[3.8766403 3.4932933]\n",
      "iter_count: 105，loss:0.037512082896152064, w:[3.867461 3.498599]\n",
      "iter_count: 106，loss:0.03673092500129715, w:[3.8583777 3.5038493]\n",
      "iter_count: 107，loss:0.035965596599970374, w:[3.8493896 3.5090444]\n",
      "iter_count: 108，loss:0.03521425253421912, w:[3.8404956 3.5141852]\n",
      "iter_count: 109，loss:0.0344802315784651, w:[3.8316948 3.5192723]\n",
      "iter_count: 110，loss:0.03376698552735652, w:[3.8229861 3.5243058]\n",
      "iter_count: 111，loss:0.033058322246116954, w:[3.8143687 3.5292869]\n",
      "iter_count: 112，loss:0.03237190855742083, w:[3.8058414 3.5342157]\n",
      "iter_count: 113，loss:0.031696920277478426, w:[3.7974036 3.539093 ]\n",
      "iter_count: 114，loss:0.03103976886975724, w:[3.789054 3.543919]\n",
      "iter_count: 115，loss:0.03039079126477045, w:[3.7807918 3.5486946]\n",
      "iter_count: 116，loss:0.029755835439218573, w:[3.7726161 3.5534203]\n",
      "iter_count: 117，loss:0.029138261049491486, w:[3.7645261 3.5580964]\n",
      "iter_count: 118，loss:0.028531157648310542, w:[3.7565207 3.5627236]\n",
      "iter_count: 119，loss:0.027937477957550437, w:[3.7485993 3.5673022]\n",
      "iter_count: 120，loss:0.02735402098299382, w:[3.7407608 3.571833 ]\n",
      "iter_count: 121，loss:0.026783044778131083, w:[3.7330043 3.5763164]\n",
      "iter_count: 122，loss:0.02622790653597658, w:[3.725329  3.5807526]\n",
      "iter_count: 123，loss:0.025677444957680196, w:[3.717734  3.5851426]\n",
      "iter_count: 124，loss:0.025144597237376734, w:[3.7102187 3.5894866]\n",
      "iter_count: 125，loss:0.024620610382553424, w:[3.702782 3.593785]\n",
      "iter_count: 126，loss:0.024106594582373513, w:[3.6954231 3.5980387]\n",
      "iter_count: 127，loss:0.023607571558477144, w:[3.6881413 3.6022475]\n",
      "iter_count: 128，loss:0.023113170404424087, w:[3.6809359 3.6064124]\n",
      "iter_count: 129，loss:0.022633475259697295, w:[3.6738057 3.6105337]\n",
      "iter_count: 130，loss:0.02216218344746485, w:[3.6667502 3.6146119]\n",
      "iter_count: 131，loss:0.021700334282801492, w:[3.6597686 3.6186473]\n",
      "iter_count: 132，loss:0.02124874734377954, w:[3.6528602 3.6226404]\n",
      "iter_count: 133，loss:0.020804346141827777, w:[3.646024  3.6265917]\n",
      "iter_count: 134，loss:0.020369731562651937, w:[3.6392596 3.6305017]\n",
      "iter_count: 135，loss:0.019948324287211654, w:[3.6325657 3.6343708]\n",
      "iter_count: 136，loss:0.01953179086122, w:[3.6259422 3.6381993]\n",
      "iter_count: 137，loss:0.019126341117371704, w:[3.6193879 3.6419876]\n",
      "iter_count: 138，loss:0.018723457851111014, w:[3.6129024 3.6457365]\n",
      "iter_count: 139，loss:0.018337408437627895, w:[3.6064847 3.6494458]\n",
      "iter_count: 140，loss:0.017951996744150527, w:[3.6001341 3.6531165]\n",
      "iter_count: 141，loss:0.01757842086313794, w:[3.5938501 3.6567488]\n",
      "iter_count: 142，loss:0.017213697452134514, w:[3.587632 3.660343]\n",
      "iter_count: 143，loss:0.016854877495032634, w:[3.5814788 3.6638994]\n",
      "iter_count: 144，loss:0.01650274759094609, w:[3.57539   3.6674187]\n",
      "iter_count: 145，loss:0.016158233349823057, w:[3.5693653 3.6709013]\n",
      "iter_count: 146，loss:0.015824037019658496, w:[3.5634034 3.6743472]\n",
      "iter_count: 147，loss:0.015491766065243063, w:[3.557504  3.6777573]\n",
      "iter_count: 148，loss:0.015171538205760591, w:[3.5516663 3.6811314]\n",
      "iter_count: 149，loss:0.01485352492332604, w:[3.5458899 3.6844704]\n",
      "iter_count: 150，loss:0.014547616065698456, w:[3.5401738 3.6877742]\n",
      "iter_count: 151，loss:0.01424175793773029, w:[3.5345175 3.6910436]\n",
      "iter_count: 152，loss:0.013946054502599873, w:[3.5289207 3.6942787]\n",
      "iter_count: 153，loss:0.013656319477977012, w:[3.5233822 3.69748  ]\n",
      "iter_count: 154，loss:0.013371178466246055, w:[3.517902  3.7006476]\n",
      "iter_count: 155，loss:0.013092155530161928, w:[3.5124788 3.703782 ]\n",
      "iter_count: 156，loss:0.01281808066890162, w:[3.5071127 3.706884 ]\n",
      "iter_count: 157，loss:0.012553496914007702, w:[3.5018027 3.709953 ]\n",
      "iter_count: 158，loss:0.012290515728254467, w:[3.4965484 3.7129903]\n",
      "iter_count: 159，loss:0.012037197943254432, w:[3.491349  3.7159956]\n",
      "iter_count: 160，loss:0.011785646198171981, w:[3.486204  3.7189693]\n",
      "iter_count: 161，loss:0.011538682704816984, w:[3.481113 3.721912]\n",
      "iter_count: 162，loss:0.011297547265133835, w:[3.4760752 3.7248237]\n",
      "iter_count: 163，loss:0.011061078356788493, w:[3.4710903 3.7277052]\n",
      "iter_count: 164，loss:0.010833603256443985, w:[3.4661574 3.7305565]\n",
      "iter_count: 165，loss:0.01060788258252578, w:[3.4612763 3.7333777]\n",
      "iter_count: 166，loss:0.01038507660573814, w:[3.4564464 3.7361696]\n",
      "iter_count: 167，loss:0.010170730271188314, w:[3.4516668 3.7389321]\n",
      "iter_count: 168，loss:0.009957775913135358, w:[3.4469373 3.7416658]\n",
      "iter_count: 169，loss:0.009750443729065956, w:[3.4422574 3.744371 ]\n",
      "iter_count: 170，loss:0.009548022951177338, w:[3.4376266 3.7470477]\n",
      "iter_count: 171，loss:0.009349995631966975, w:[3.4330442 3.7496963]\n",
      "iter_count: 172，loss:0.009154211336025037, w:[3.4285097 3.7523172]\n",
      "iter_count: 173，loss:0.008963082448332351, w:[3.4240227 3.7549107]\n",
      "iter_count: 174，loss:0.008776021076738836, w:[3.4195828 3.757477 ]\n",
      "iter_count: 175，loss:0.008593969817216565, w:[3.4151893 3.7600164]\n",
      "iter_count: 176，loss:0.008413384020616014, w:[3.410842  3.7625294]\n",
      "iter_count: 177，loss:0.008239638972111062, w:[3.40654   3.7650158]\n",
      "iter_count: 178，loss:0.008066420144169025, w:[3.402283  3.7674763]\n",
      "iter_count: 179，loss:0.00789734650406208, w:[3.3980708 3.7699113]\n",
      "iter_count: 180，loss:0.007736249437812148, w:[3.3939025 3.7723205]\n",
      "iter_count: 181，loss:0.007574255878833719, w:[3.389778  3.7747045]\n",
      "iter_count: 182，loss:0.00741491742336084, w:[3.3856966 3.7770636]\n",
      "iter_count: 183，loss:0.007262068584565077, w:[3.381658 3.779398]\n",
      "iter_count: 184，loss:0.0071110222847524706, w:[3.3776617 3.7817078]\n",
      "iter_count: 185，loss:0.006961386722996031, w:[3.3737073 3.7839935]\n",
      "iter_count: 186，loss:0.006816678844200306, w:[3.3697941 3.7862554]\n",
      "iter_count: 187，loss:0.006674975831651046, w:[3.365922  3.7884934]\n",
      "iter_count: 188，loss:0.006534788922116514, w:[3.3620903 3.790708 ]\n",
      "iter_count: 189，loss:0.006398222474513115, w:[3.358299  3.7928996]\n",
      "iter_count: 190，loss:0.006265974015592656, w:[3.3545473 3.7950683]\n",
      "iter_count: 191，loss:0.006136528965328125, w:[3.3508348 3.797214 ]\n",
      "iter_count: 192，loss:0.006008293039459751, w:[3.3471613 3.7993374]\n",
      "iter_count: 193，loss:0.00588314467072487, w:[3.3435261 3.8014386]\n",
      "iter_count: 194，loss:0.005760762458365297, w:[3.339929  3.8035176]\n",
      "iter_count: 195，loss:0.005639814104102789, w:[3.3363698 3.805575 ]\n",
      "iter_count: 196，loss:0.005522048022281616, w:[3.3328476 3.8076108]\n",
      "iter_count: 197，loss:0.00540715271935369, w:[3.3293624 3.8096254]\n",
      "iter_count: 198，loss:0.005295939892582829, w:[3.3259137 3.8116188]\n",
      "iter_count: 199，loss:0.005185786530846962, w:[3.322501  3.8135912]\n",
      "iter_count: 200，loss:0.005076413012651392, w:[3.319124  3.8155432]\n",
      "iter_count: 201，loss:0.0049707854663102805, w:[3.3157825 3.8174746]\n",
      "iter_count: 202，loss:0.004867646964848974, w:[3.312476  3.8193858]\n",
      "iter_count: 203，loss:0.004765380363460645, w:[3.309204  3.8212771]\n",
      "iter_count: 204，loss:0.004668074284520481, w:[3.3059664 3.8231485]\n",
      "iter_count: 205，loss:0.0045700955512034845, w:[3.3027625 3.8250003]\n",
      "iter_count: 206，loss:0.004474301617547462, w:[3.2995923 3.8268328]\n",
      "iter_count: 207，loss:0.004381500476232851, w:[3.2964551 3.828646 ]\n",
      "iter_count: 208，loss:0.004289327056936235, w:[3.293351  3.8304403]\n",
      "iter_count: 209，loss:0.004200563485940166, w:[3.2902792 3.8322158]\n",
      "iter_count: 210，loss:0.004112912686153232, w:[3.2872396 3.8339727]\n",
      "iter_count: 211，loss:0.004027145823289538, w:[3.284232  3.8357112]\n",
      "iter_count: 212，loss:0.003943994184246185, w:[3.2812557 3.8374314]\n",
      "iter_count: 213，loss:0.00386129276605825, w:[3.2783108 3.8391337]\n",
      "iter_count: 214，loss:0.003781646622881817, w:[3.2753966 3.8408182]\n",
      "iter_count: 215，loss:0.0037028351275806928, w:[3.272513 3.842485]\n",
      "iter_count: 216，loss:0.003625657430793217, w:[3.2696593 3.8441343]\n",
      "iter_count: 217，loss:0.0035497743453921427, w:[3.2668357 3.8457663]\n",
      "iter_count: 218，loss:0.0034751660053865636, w:[3.2640417 3.8473814]\n",
      "iter_count: 219，loss:0.0034033248379478206, w:[3.261277  3.8489795]\n",
      "iter_count: 220，loss:0.003333074710059214, w:[3.258541  3.8505607]\n",
      "iter_count: 221，loss:0.003262542842360654, w:[3.2558339 3.8521254]\n",
      "iter_count: 222，loss:0.003194048423313598, w:[3.253155 3.853674]\n",
      "iter_count: 223，loss:0.003128314178452479, w:[3.2505043 3.855206 ]\n",
      "iter_count: 224，loss:0.0030626300370553508, w:[3.2478812 3.856722 ]\n",
      "iter_count: 225，loss:0.0029984067500339736, w:[3.2452857 3.8582225]\n",
      "iter_count: 226，loss:0.0029371940222836434, w:[3.2427173 3.8597069]\n",
      "iter_count: 227，loss:0.002874574131684858, w:[3.2401757 3.861176 ]\n",
      "iter_count: 228，loss:0.00281532943770892, w:[3.237661  3.8626297]\n",
      "iter_count: 229，loss:0.0027569089329630516, w:[3.2351723 3.864068 ]\n",
      "iter_count: 230，loss:0.002699056346434395, w:[3.23271   3.8654914]\n",
      "iter_count: 231，loss:0.002643310504878537, w:[3.2302732 3.8668997]\n",
      "iter_count: 232，loss:0.002587860125927932, w:[3.2278621 3.8682935]\n",
      "iter_count: 233，loss:0.002534822725995036, w:[3.225476  3.8696725]\n",
      "iter_count: 234，loss:0.0024809438927695738, w:[3.2231152 3.8710372]\n",
      "iter_count: 235，loss:0.0024300601221972327, w:[3.220779  3.8723876]\n",
      "iter_count: 236，loss:0.0023795500601772805, w:[3.2184672 3.8737237]\n",
      "iter_count: 237，loss:0.0023294286953052736, w:[3.2161796 3.875046 ]\n",
      "iter_count: 238，loss:0.0022808873579138887, w:[3.213916  3.8763545]\n",
      "iter_count: 239，loss:0.002233895065728575, w:[3.2116761 3.877649 ]\n",
      "iter_count: 240，loss:0.002186812108269442, w:[3.2094598 3.8789303]\n",
      "iter_count: 241，loss:0.0021423638912915523, w:[3.2072666 3.880198 ]\n",
      "iter_count: 242，loss:0.0020976154703475913, w:[3.2050962 3.8814523]\n",
      "iter_count: 243，loss:0.002053083079445264, w:[3.2029488 3.8826938]\n",
      "iter_count: 244，loss:0.002011468299315311, w:[3.2008235 3.8839219]\n",
      "iter_count: 245，loss:0.0019677422205688576, w:[3.1987207 3.8851373]\n",
      "iter_count: 246，loss:0.001926677798816763, w:[3.19664   3.8863401]\n",
      "iter_count: 247，loss:0.0018874411503962618, w:[3.194581  3.8875303]\n",
      "iter_count: 248，loss:0.0018485163013218881, w:[3.1925435 3.8887079]\n",
      "iter_count: 249，loss:0.0018091653657255847, w:[3.1905274 3.8898733]\n",
      "iter_count: 250，loss:0.0017719162253433752, w:[3.1885324 3.8910265]\n",
      "iter_count: 251，loss:0.0017352666822276661, w:[3.1865582 3.8921676]\n",
      "iter_count: 252，loss:0.0016992104438586697, w:[3.1846046 3.8932967]\n",
      "iter_count: 253，loss:0.0016635623076604136, w:[3.1826715 3.894414 ]\n",
      "iter_count: 254，loss:0.0016282916965952607, w:[3.180759  3.8955197]\n",
      "iter_count: 255，loss:0.0015958084890773765, w:[3.1788661 3.8966136]\n",
      "iter_count: 256，loss:0.0015615991083805284, w:[3.1769931 3.8976963]\n",
      "iter_count: 257，loss:0.001529254393861629, w:[3.17514   3.8987675]\n",
      "iter_count: 258，loss:0.001497529000375016, w:[3.173306  3.8998275]\n",
      "iter_count: 259，loss:0.001466280025353717, w:[3.1714914 3.9008763]\n",
      "iter_count: 260，loss:0.0014354779619481861, w:[3.1696956 3.9019141]\n",
      "iter_count: 261，loss:0.0014049442462743175, w:[3.1679187 3.9029412]\n",
      "iter_count: 262，loss:0.0013757665173721992, w:[3.1661603 3.9039576]\n",
      "iter_count: 263，loss:0.0013472915341714042, w:[3.1644206 3.9049633]\n",
      "iter_count: 264，loss:0.0013195092139363855, w:[3.162699  3.9059584]\n",
      "iter_count: 265，loss:0.001292223046963136, w:[3.1609952 3.906943 ]\n",
      "iter_count: 266，loss:0.0012648179803568381, w:[3.1593094 3.9079175]\n",
      "iter_count: 267，loss:0.001238349561543237, w:[3.1576414 3.9088817]\n",
      "iter_count: 268，loss:0.001212799276766964, w:[3.1559908 3.9098358]\n",
      "iter_count: 269，loss:0.001187986974715591, w:[3.1543574 3.91078  ]\n",
      "iter_count: 270，loss:0.0011632892944508965, w:[3.1527412 3.911714 ]\n",
      "iter_count: 271，loss:0.001138731324906098, w:[3.151142  3.9126384]\n",
      "iter_count: 272，loss:0.0011147231693848882, w:[3.1495593 3.9135532]\n",
      "iter_count: 273，loss:0.0010916617005686023, w:[3.1479933 3.9144585]\n",
      "iter_count: 274，loss:0.001069532933642131, w:[3.1464436 3.915354 ]\n",
      "iter_count: 275，loss:0.0010462335290526426, w:[3.1449103 3.9162405]\n",
      "iter_count: 276，loss:0.0010252105626455887, w:[3.143393  3.9171174]\n",
      "iter_count: 277，loss:0.0010034313047759724, w:[3.1418915 3.9179852]\n",
      "iter_count: 278，loss:0.000982245104969479, w:[3.140406 3.918844]\n",
      "[3.140406 3.918844]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gen_line_data(sample_num=100):\n",
    "    \"\"\"\n",
    "    y = 3*x1 + 4*x2\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    x1 = np.linspace(0, 9, sample_num)\n",
    "    x2 = np.linspace(4, 13, sample_num)\n",
    "    x = np.concatenate(([x1], [x2]), axis=0).T\n",
    "    y = np.dot(x, np.array([3, 4]).T)  # y 列向量\n",
    "    return x, y\n",
    "\n",
    "def bgd(samples, y, step_size=0.01, max_iter_count=10000):\n",
    "    sample_num, dim = samples.shape\n",
    "    y = y.flatten()\n",
    "    w = np.ones((dim,), dtype=np.float32)*10\n",
    "    loss = 10\n",
    "    iter_count = 0\n",
    "    while loss > 0.001 and iter_count < max_iter_count:\n",
    "        loss = 0\n",
    "        error = np.zeros((dim,), dtype=np.float32)\n",
    "        for i in range(sample_num):\n",
    "            predict_y = np.dot(w.T, samples[i])\n",
    "            for j in range(dim):\n",
    "                error[j] += (y[i] - predict_y ) * samples[i][j]   # 计算误差，梯度\n",
    "\n",
    "        for j in range(dim):\n",
    "            w[j] += step_size * error[j] / sample_num\n",
    "        loss=((1 / (sample_num * dim))*np.sum(error)) ** 2\n",
    "#         for i in range(sample_num):\n",
    "#             predict_y = np.dot(w.T, samples[i])\n",
    "#             error = (1 / (sample_num * dim)) * np.power((predict_y - y[i]), 2)\n",
    "#             loss += error\n",
    "\n",
    "        print(\"iter_count: {}，loss:{}, w:{}\".format(iter_count, loss, w))\n",
    "        iter_count += 1\n",
    "    return w\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    samples, y = gen_line_data()\n",
    "    w = bgd(samples, y)\n",
    "    print(w)  # 会很接近[3, 4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
